<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>激活函数</title>
      <link href="/2023/05/13/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
      <url>/2023/05/13/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>【1】</p><p>将输出层值经过一个非线性函数实现增加非线性化的操作</p><p>【2】</p><p>刚开始假设的模型为线性模型，而线性模型的叠加仍是线性模型；</p><p>非线性化的操作可以拟合更为复杂的模型。</p><p><img src="image-20230513144230094.png" alt="image-20230513144230094" style="zoom: 50%;" /></p><script type="math/tex; mode=display">\begin{aligned}y_j &= \sum_{i=1}^{m}(x_iw_{ji}) + b_j \\z_k &= \sum_{j=1}^{n}(y_jw_{kj}) + b_k \\    &= \sum_{j=1}^{n}(w_{kj}(\sum_{i=1}^{m}(x_iw_{ji})+b_j) + b_k) \\    &= b_k + \sum_{j=1}^nw_{kj}b_j + \sum_{i=1}^m\sum_{j=1}^nx_iw_{ji}w_{jk}\end{aligned}</script><p>【3】</p><p>Sigmoid</p><p><img src="image-20230513150611516.png" alt="image-20230513150611516" style="zoom:33%;" /></p><script type="math/tex; mode=display">Sigmoid(x) = \frac {1}{1 + e ^ {-x}}</script><p>E.g.<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Sigmoid()</span><br><span class="line">input = torch.randn(2)</span><br><span class="line">output = m(input)</span><br></pre></td></tr></table></figure></p><p>ReLU</p><p><img src="image-20230513150644075.png" alt="image-20230513150644075" style="zoom:33%;" /></p><script type="math/tex; mode=display">ReLU(x)=max(0,x)</script><p>E.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = nn.ReLU()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">2</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure><p>LeakyReLU</p><p><img src="image-20230513151210948.png" alt="image-20230513151210948" style="zoom:33%;" /></p><script type="math/tex; mode=display">LeakyReLU(x)=max(0,x)+negative_slope∗min(0,x)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = nn.LeakyReLU(<span class="number">0.1</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">2</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>损失函数</title>
      <link href="/2023/05/13/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
      <url>/2023/05/13/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>【1】 用于衡量模型预测值与真实值偏差的手段</p><p>【2】通过计算得出的偏差，不断校准模型</p><p>【3】</p><p><strong>MAE/L1 Norm</strong></p><p>平均绝对损失、L1范数；用于回归问题</p><script type="math/tex; mode=display">\mathcal L_1(\bar y,y) = |\bar y- y|</script><p>E.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.L1Loss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure><p><strong>MSE/L2 Norm</strong></p><p>平均平方损失、L2范数；用于回归问题</p><script type="math/tex; mode=display">\mathcal L_2(\bar y,y) = (\bar y - y) ^2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure><p><strong>CE</strong></p><p>交叉熵损失；用于多分类问题（是哪个）</p><script type="math/tex; mode=display">\mathcal L_{CE} (\bar y,y) = -ylog(\bar y)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of target with class indices</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br><span class="line"><span class="comment"># Example of target with class probabilities</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>, <span class="number">5</span>).softmax(dim=<span class="number">1</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure><p><strong>BCE</strong></p><p>二元交叉熵损失。用于分类问题(是或不是)</p><script type="math/tex; mode=display">\mathcal L_{BCE}(\bar{y},y)=−ylog(\bar{y})-(1-y)log(1-\bar{y})</script><p>E.g.</p><script type="math/tex; mode=display">\mathcal L_G =−E_{z∼p(z)}logD(G(z))</script><script type="math/tex; mode=display">\mathcal L_D =−E_{x∼p_{data}}logD(x)−E_{z∼p(z)}log(1−D(G(z)))</script><p>等价于</p><script type="math/tex; mode=display">\mathcal L_G = \mathcal L_{BCE}(D(G(z)),1)</script><script type="math/tex; mode=display">\mathcal L_D= \mathcal L_{BCE}(D(x),1) + \mathcal L_{BCE}(D(G(z)),0)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### LG</span></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line">z = torch.randn()</span><br><span class="line">label_g = torch.full(<span class="number">1</span>)</span><br><span class="line">pred_label_g = D(G(z))</span><br><span class="line">loss_g = criterion(pred_label_g,label_g)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>池化层</title>
      <link href="/2023/05/12/%E6%B1%A0%E5%8C%96%E5%B1%82/"/>
      <url>/2023/05/12/%E6%B1%A0%E5%8C%96%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<p>【1】</p><p>一种下采样操作。比如可以使图片分辨率变小。</p><p><img src="image-20230512152644339.png" alt="image-20230512152644339" style="zoom: 25%;" /></p><p>【2】</p><p>池化用于特征图压缩，让训练参数变少的同时，提取主要特征。</p><p>【3】</p><p>实现池化的方式有两种，一是使用步长不为一的卷积、二是直接采样。</p><p>使用步长S不为一的卷积：一般情况分子变小而分母变大，固总体变小，即H、W变小</p><script type="math/tex; mode=display">H_{out} = \frac{H_{in} + 2P - K}{S} + 1</script><p>直接采样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.MaxPool2d(kernel_size, stride)</span><br></pre></td></tr></table></figure><p>E.g.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pool of square window of size=3, stride=2</span></span><br><span class="line">m = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># pool of non-square window</span></span><br><span class="line">m = nn.MaxPool2d((<span class="number">3</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>, <span class="number">32</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>全连接层</title>
      <link href="/2023/05/12/%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82/"/>
      <url>/2023/05/12/%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<p>【1】</p><p>一种联立前一层所有神经元的技术</p><p><img src="image-20230512150856372.png" alt="image-20230512150856372" style="zoom:50%;" /></p><p>【2】</p><p>可以用于融合所有的层次的特征。</p><p>其缺点是需要巨大参数量计算以及结构信息的缺失（如平移不变性，需要调整额外参数学习这种特性）</p><p>【3】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.Linear(in_features,out_features)</span><br></pre></td></tr></table></figure><p>E.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Linear(<span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">128</span>, <span class="number">20</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>lc77</title>
      <link href="/2023/05/10/lc77/"/>
      <url>/2023/05/10/lc77/</url>
      
        <content type="html"><![CDATA[<p>【题目】<a href="https://leetcode.cn/problems/combinations/description/">lc77</a></p><p>【题意】</p><p><img src="image-20230510180711186.png" alt="image-20230510180711186" style="zoom:33%;" /></p><p>【题解】</p><p><img src="image-20230510180811165.png" alt="image-20230510180811165" style="zoom: 50%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回溯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>转置卷积（Transpose Convolution）</title>
      <link href="/2023/05/10/%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%EF%BC%88Transpose-Convolution%EF%BC%89/"/>
      <url>/2023/05/10/%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%EF%BC%88Transpose-Convolution%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>【1】</p><p>卷积的”逆”过程，是一种上采样操作</p><p><img src="Untitled.png" alt="Untitled" style="zoom:50%;" /></p><p>【2】</p><p>通过相对较小的特征图恢复原分辨率大小，可以用来对比学习这些参数的作用</p><p>【3】</p><p>双线性插值：</p><p>转置卷积：</p><script type="math/tex; mode=display">H_{out} = (H_{in} - 1) * S + K - 2P  + 1\tag{1}</script><script type="math/tex; mode=display">W_{out} = (W_{in} - 1) * S + K - 2P + 1\tag{2}</script><p>下采样时的卷积步长大于1，则反卷积时需要设置output_padding</p><script type="math/tex; mode=display">H_{out}=(H_{in}−1)×S−2P+dilation[0]×(K−1)+outputpadding[0]+1</script><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">16</span>, <span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line">downsample = nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, <span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">upsample = nn.ConvTranspose2d(<span class="number">16</span>, <span class="number">16</span>, <span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># torch.Size([1, 16, 6, 6])</span></span><br><span class="line">h = downsample(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># torch.Size([1, 16, 12, 12])</span></span><br><span class="line">output = upsample(h, output_size=<span class="built_in">input</span>.size())</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>卷积层</title>
      <link href="/2023/05/10/%E6%99%AE%E9%80%9A%E5%8D%B7%E7%A7%AF/"/>
      <url>/2023/05/10/%E6%99%AE%E9%80%9A%E5%8D%B7%E7%A7%AF/</url>
      
        <content type="html"><![CDATA[<p>【1】</p><p>指的是作用于图像的二维卷积，它是一种提取保留邻域信息或下采样的操作</p><p>【2】</p><p>用于提取图像局部特征，如纹理，颜色，深度等；<br>其优点是权重共享方式（共用一个卷积核），可以等价于实现平移不变性且参数相对较少。</p><p>【3】</p><p>卷积核K：</p><p><img src="Untitled.png" alt="Untitled"></p><p>填充P：</p><p><img src="Untitled-16837099936742.png" alt="Untitled" style="zoom:33%;" /></p><p>步幅S：</p><p><img src="Untitled-16837100168054.png" alt="Untitled" style="zoom:50%;" /></p><p><img src="Untitled-16837100302296.png" alt="Untitled" style="zoom:50%;" /></p><script type="math/tex; mode=display">H_{out} = \frac{H_{in} + 2P - K}{S} + 1 \tag{1}</script><script type="math/tex; mode=display">W_{out} = \frac{W_{in} + 2P - K}{S} + 1 \tag{2}</script><p>注意到当P = K//2 时，保持特征图的大小不变，只改变通道数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">64</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">3</span>//<span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.Conv2d(in_channels, out_channels, kernel_size, padding)</span><br></pre></td></tr></table></figure><p>E.g.<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># With square kernels and equal stride</span><br><span class="line">m = nn.Conv2d(16, 33, 3, stride=2)</span><br><span class="line"># non-square kernels and unequal stride and with padding</span><br><span class="line">m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))</span><br><span class="line"># non-square kernels and unequal stride and with padding and dilation</span><br><span class="line">m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))</span><br><span class="line">input = torch.randn(20, 16, 50, 100)</span><br><span class="line">output = m(input)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED问题</title>
      <link href="/2023/05/10/RuntimeError-cuDNN-error-CUDNN-STATUS-NOT-INITIALIZED%E9%97%AE%E9%A2%98/"/>
      <url>/2023/05/10/RuntimeError-cuDNN-error-CUDNN-STATUS-NOT-INITIALIZED%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>表示显存不足<br><code>nvidia-smi</code> 查看当前显存占用进程及其pid<br><code>kill</code> 清理一些的占用显存的进程或者改batch_size</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo加载公式问题</title>
      <link href="/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%85%AC%E5%BC%8F%E9%97%AE%E9%A2%98/"/>
      <url>/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%85%AC%E5%BC%8F%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/yexiaohhjk/article/details/82526604">配置方法参考</a></p><p>配置后正常显示公式</p><p><img src="image-20230509153716208.png" alt="image-20230509153716208"></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hexo命令操作</title>
      <link href="/2023/05/09/hexo%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
      <url>/2023/05/09/hexo%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p><code>hexo new post &quot; &quot;</code>：创建一篇博客</p><p><code>hexo cl</code>：清理public文件夹</p><p><code>hexo g</code>：生成public文件夹</p><p><code>hexo s</code>：启动博客</p><p><code>hexo deploy</code>：本地修改同步至远端</p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hexo加载图片问题</title>
      <link href="/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/"/>
      <url>/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/476601594">配置方法参考</a></p><p><strong>1、配置图像根目录</strong></p><p><img src="image-20230509151512393.png" alt="image-20230509151512393" style="zoom:50%;" /></p><p><strong>2、删除图像名前面的斜杠</strong></p><p><img src="image-20230509151638033.png" alt="image-20230509151638033" style="zoom:67%;" /></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>反向传播</title>
      <link href="/2023/05/09/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
      <url>/2023/05/09/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
      
        <content type="html"><![CDATA[<p>【1】</p><p>是一种更新训练参数（权重）的技术</p><p>【2】</p><p>从后向前逐层更新参数，但是后层的梯度值需要前层计算所得</p><p>【3】</p><p><img src="image-20230509135540661.png" alt="image-20230509135540661" style="zoom:50%;" /></p><script type="math/tex; mode=display">\begin{aligned}\sigma (·) &: Activation \, function \\f(·)&: Fully\,connected\,layer \\x^l_j &: 第l层上的第j个结点的值 \\ w_{ji}^{l} &: 第l-1层的第i个节点到第l层的第j个结点的权重\end{aligned}\tag{0}</script><script type="math/tex; mode=display">\begin{aligned}x^l_j &= \sum_{i=1} ^{m} w_{ji}^{l}y_{i}^{l-1} +  \theta_{j}^{l} \end{aligned} \tag{1}</script><script type="math/tex; mode=display">\begin{aligned}y_{j} ^ {l} &= \sigma (x_{j}^{l}) \end{aligned} \tag{2}</script><script type="math/tex; mode=display">\bar{y} = f(\sum_{j=1}^{n}y_{j}) \tag{3}</script><script type="math/tex; mode=display">\mathcal L = |y - \bar{y}| \tag{4}</script><p>改变某一个位置的权重，对整体损失函数的影响</p><script type="math/tex; mode=display">\begin{aligned}\frac{ \partial \mathcal L}{\partial w_{ji}^{l}} &= \frac{\partial \mathcal L}{\partial x^l_j} \frac{\partial x^l_j}{\partial w_{ji}^{l}}\end{aligned} \tag{5}</script><p>因此这种连乘的形式给深层网络带来了梯度消失/爆炸问题</p><script type="math/tex; mode=display">\begin{aligned}w_{ji}^{l} = w_{ji}^{l}  - \mathcal \alpha \frac{\partial \mathcal L}{\partial w_{ji}^{l}}\end{aligned} \tag{6}</script><p>导致最终很难持续学习下去</p><p>E.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
