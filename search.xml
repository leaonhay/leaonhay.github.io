<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【implementation】SimAM</title>
      <link href="/2024/08/15/%E3%80%90ICML-2021%E3%80%91SimAM/"/>
      <url>/2024/08/15/%E3%80%90ICML-2021%E3%80%91SimAM/</url>
      
        <content type="html"><![CDATA[<h1 id="【ICML-2021】SimAM"><a href="#【ICML-2021】SimAM" class="headerlink" title="【ICML-2021】SimAM"></a>【ICML-2021】SimAM</h1><h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><p>能量函数</p><p><img src="4485e9ec-3d99-467b-9655-e7d71e61ad8d.png" alt="image.png"></p><p>最小化能量函数的方程</p><p><img src="image.png" alt="image.png"></p><p>最终的输出结果</p><p><img src="image%201.png" alt="image.png"></p><script type="math/tex; mode=display">\begin{aligned}\frac{1}{e} &= \frac{(t-\hat \mu)^2}{4(\hat \sigma^2 + \lambda)} + \frac{2(\hat \sigma^2 + \lambda)}{4(\hat \sigma^2 + \lambda)} &= \frac{(t-\hat \mu)^2}{4(\hat \sigma^2 + \lambda)} + \frac {1}{2}\end{aligned}</script><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimAM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,Lambda=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param Lambda: 能量函数里面的超参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SimAM, self).__init__()</span><br><span class="line">        self.Lambda = Lambda</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        n,c,h,w = x.shape</span><br><span class="line">        M = h * w - <span class="number">1</span></span><br><span class="line">        t_minus_mu_square = x - x.mean(dim=[<span class="number">2</span>,<span class="number">3</span>],keepdim = <span class="literal">True</span>).<span class="built_in">pow</span>(<span class="number">2</span>)</span><br><span class="line">        sigma_square = torch.<span class="built_in">sum</span>(t_minus_mu_square) / M</span><br><span class="line">        one_divide_e = t_minus_mu_square / (<span class="number">4</span> * sigma_square + self.Lambda) + <span class="number">0.5</span></span><br><span class="line">        out = F.sigmoid(one_divide_e) * x</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.randn(<span class="number">2</span>,<span class="number">8</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;iutput shape:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(x.shape))</span><br><span class="line">    model = SimAM()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output shape:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(model(x).shape))</span><br></pre></td></tr></table></figure><h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【implementation】Coordinate Attention</title>
      <link href="/2024/08/15/%E3%80%90CVPR-21%E3%80%91Coordinate%20Attention%20ee9782e0c6ce4f7a86623fbcc0f3790f/"/>
      <url>/2024/08/15/%E3%80%90CVPR-21%E3%80%91Coordinate%20Attention%20ee9782e0c6ce4f7a86623fbcc0f3790f/</url>
      
        <content type="html"><![CDATA[<h1 id="【CVPR-21】Coordinate-Attention"><a href="#【CVPR-21】Coordinate-Attention" class="headerlink" title="【CVPR-21】Coordinate Attention"></a>【CVPR-21】Coordinate Attention</h1><h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><p><img src="image.png" alt="image.png"></p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CoordinateAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_ch,r</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param in_ch: 特征图输入通道数</span></span><br><span class="line"><span class="string">        :param r: 特征图通道数的减少比率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(CoordinateAttention, self).__init__()</span><br><span class="line">        self.x_avg_pool = nn.AdaptiveAvgPool2d((<span class="literal">None</span>,<span class="number">1</span>))</span><br><span class="line">        self.y_avg_pool = nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="literal">None</span>))</span><br><span class="line">        self.conv_1x1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=in_ch, out_channels=in_ch // r, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">        )</span><br><span class="line">        self.act = nn.Sigmoid()</span><br><span class="line">        self.non_linear = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(in_ch // r),</span><br><span class="line">        )</span><br><span class="line">        self.conv_x = nn.Conv2d(</span><br><span class="line">            in_channels=in_ch//r,out_channels=in_ch,kernel_size=<span class="number">1</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span></span><br><span class="line">        )</span><br><span class="line">        self.conv_y = nn.Conv2d(</span><br><span class="line">            in_channels=in_ch // r, out_channels=in_ch, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        n,c,h,w = x.shape</span><br><span class="line">        <span class="comment"># [N,C,H,W] -&gt; [N,C,H,1] -&gt; [N,C,1,H]</span></span><br><span class="line">        self.x_direction = self.x_avg_pool(x).transpose(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># [N,C,H,W] -&gt; [N,C,1,W]</span></span><br><span class="line">        self.y_direction = self.y_avg_pool(x)</span><br><span class="line">        <span class="comment"># [N,C,1,H+W]</span></span><br><span class="line">        out = self.conv_1x1(torch.cat([self.x_direction,self.y_direction],dim=<span class="number">3</span>))</span><br><span class="line">        out = self.non_linear(out)</span><br><span class="line">        <span class="comment"># [N,C,1,H+W] -&gt; [N,C,1,H] + [N,C,1,W]</span></span><br><span class="line">        self.x,self.y = torch.split(out,[h,w],dim=<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># [N,C,1,H] -&gt; [N,C,H,1]</span></span><br><span class="line">        self.x = self.x.transpose(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">        out_x = F.sigmoid(self.conv_x(self.x))</span><br><span class="line">        out_y = F.sigmoid(self.conv_y(self.y))</span><br><span class="line">        <span class="comment"># [N,C,H,W] * [N,C,H,1] * [N,C,1,W]</span></span><br><span class="line">        <span class="keyword">return</span> x * out_x * out_y</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.randn(<span class="number">2</span>,<span class="number">16</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;input&#123;&#125;&quot;</span>.<span class="built_in">format</span>(x.shape))</span><br><span class="line">    N,C,H,W = x.shape</span><br><span class="line">    network = CoordinateAttention(C,H,W,<span class="number">8</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;onput&#123;&#125;&quot;</span>.<span class="built_in">format</span>(network(x).shape))</span><br></pre></td></tr></table></figure><h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><h2 id="对不同通道单独进行池化操作"><a href="#对不同通道单独进行池化操作" class="headerlink" title="对不同通道单独进行池化操作"></a>对不同通道单独进行池化操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只对W通道池化</span></span><br><span class="line">nn.AdaptiveAvgPool2d((<span class="literal">None</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 只对H通道池化</span></span><br><span class="line">nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="literal">None</span>))</span><br></pre></td></tr></table></figure><h2 id="对一个张量沿着某个通道拆分为两份"><a href="#对一个张量沿着某个通道拆分为两份" class="headerlink" title="对一个张量沿着某个通道拆分为两份"></a>对一个张量沿着某个通道拆分为两份</h2><p>沿着dim=3维度拆分张量为两部分，第一个部分大小为h，第二个部分大小为w</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [N,C,1,H+W] -&gt; [N,C,1,H] + [N,C,1,W]</span></span><br><span class="line">self.x,self.y = torch.split(out,[h,w],dim=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【implementation】ECANet</title>
      <link href="/2024/08/14/%E3%80%90CVPR-20%E3%80%91ECANet%205bbd9ad7ec5b42d5b7998924bd17814a/"/>
      <url>/2024/08/14/%E3%80%90CVPR-20%E3%80%91ECANet%205bbd9ad7ec5b42d5b7998924bd17814a/</url>
      
        <content type="html"><![CDATA[<h1 id="【CVPR-20】ECANet"><a href="#【CVPR-20】ECANet" class="headerlink" title="【CVPR-20】ECANet"></a>【CVPR-20】ECANet</h1><h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><p>输入F，[N,C,H,W]</p><p>经过GAP，[N,C,H,W]→[N,C,1,1]→[N,C,1]→[N,1,C]</p><p>经过卷积，[N,1,C]→[N,1,C]→[N,C,1]→[N,C,1,1]</p><p>经过逐元素相乘，[N,C,1,1]→[N,C,H,W]</p><p><img src="image.png" alt="image.png"></p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p><img src="image%201.png" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ECANet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_ch,gamma=<span class="number">2</span>,b=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ECANet, self).__init__()</span><br><span class="line">        self.gap = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.in_ch = torch.tensor(in_ch,dtype=torch.float32)</span><br><span class="line">        self.k = <span class="built_in">int</span>(<span class="built_in">abs</span>(torch.log2(self.in_ch) + b) / gamma)</span><br><span class="line">        self.k = self.k <span class="keyword">if</span> self.k % <span class="number">2</span> <span class="keyword">else</span> self.k + <span class="number">1</span></span><br><span class="line">        self.conv = nn.Conv1d(in_channels=<span class="number">1</span>,out_channels=<span class="number">1</span>,</span><br><span class="line">                              kernel_size=self.k,padding=(self.k - <span class="number">1</span>) // <span class="number">2</span>)</span><br><span class="line">        self.act = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment"># [N, C, 1, 1]</span></span><br><span class="line">        out = self.gap(x)</span><br><span class="line">        <span class="comment"># [N, 1, C]</span></span><br><span class="line">        out = out.squeeze(-<span class="number">1</span>).permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">        out = self.conv(out)</span><br><span class="line">        out = out.transpose(<span class="number">1</span>,<span class="number">2</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [N, C, 1, 1]</span></span><br><span class="line">        out = self.act(out)</span><br><span class="line">        <span class="keyword">return</span> out * x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.randn(<span class="number">2</span>,<span class="number">64</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">    N,C,H,W = x.shape</span><br><span class="line">    model = ECANet(in_ch=C)</span><br><span class="line">    <span class="built_in">print</span>((model(x)).shape)</span><br></pre></td></tr></table></figure><h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><h2 id="卷积计算出现问题"><a href="#卷积计算出现问题" class="headerlink" title="卷积计算出现问题"></a>卷积计算出现问题</h2><p><code>RuntimeError: Given groups=1, weight of size [1, 1, 3, 3], expected input[1, 2, 1, 64] to have 1 channels, but got 2 channels instead</code></p><p>输入维度为[N,1,C]，不能使用2D卷积，以下是1D卷积和2D卷积的区别</p><p><strong><code>nn.Conv1d</code></strong> 输入维度为[N,C,L]，在输入的长度（L）维度上进行卷积，适用于三维输入，处理时间序列、音频信号。</p><p><strong><code>nn.Conv2d</code></strong> 输入维度为[N,C,H,W]，在输入的高度和宽度（H、W）维度上进行卷积，适用于四维输入，处理图像信息。</p>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【implementation】CBAM</title>
      <link href="/2024/08/14/%E3%80%90ECCV-18%E3%80%91CBAM/"/>
      <url>/2024/08/14/%E3%80%90ECCV-18%E3%80%91CBAM/</url>
      
        <content type="html"><![CDATA[<h1 id="【ECCV-18】CBAM"><a href="#【ECCV-18】CBAM" class="headerlink" title="【ECCV-18】CBAM"></a>【ECCV-18】CBAM</h1><h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><p>输入特征图F（N,C,H,W）</p><p>经过CA模块转化为F’（N,C,H,W）* CA（N,C,1,1）→ （N,C,H,W）</p><p>经过SA模块转化为F’’（N,C,H,W）*SA（N,1,H,W）→（N,C,H,W）</p><p>最后残差连接</p><p><img src="image.png" alt="Untitled"></p><p><img src="image%201.png" alt="Untitled"></p><p><img src="image%202.png" alt="Untitled"></p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><h2 id="CA模块"><a href="#CA模块" class="headerlink" title="CA模块"></a>CA模块</h2><p><img src="image%201.png" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ChannelAttentionModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_ch,reduction_ratio</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param in_ch: 特征图输入通道数</span></span><br><span class="line"><span class="string">        :param reduction_ratio: 特征图通道数的减少比率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(ChannelAttentionModule, self).__init__()</span><br><span class="line">        self.max_pool = nn.AdaptiveMaxPool2d(<span class="number">1</span>)</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=in_ch,out_channels=in_ch // reduction_ratio,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=in_ch // reduction_ratio,out_channels=in_ch,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.act = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment"># [N,C,H,W] -&gt; [N,C,1,1]</span></span><br><span class="line">        max_part = self.fc2(self.fc1(self.max_pool(x)))</span><br><span class="line">        <span class="comment"># [N,C,H,W] -&gt; [N,C,1,1]</span></span><br><span class="line">        avg_part = self.fc2(self.fc1(self.avg_pool(x)))</span><br><span class="line">        <span class="comment"># [N,C,1,1] -&gt; [N,C,1,1]</span></span><br><span class="line">        out = max_part + avg_part</span><br><span class="line">        out = self.act(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.rand(<span class="number">1</span>,<span class="number">64</span>,<span class="number">16</span>,<span class="number">16</span>)</span><br><span class="line">    N,C,H,W = x.shape</span><br><span class="line">    model_1 = ChannelAttentionModule(in_ch=C,reduction_ratio=<span class="number">8</span>)</span><br><span class="line">    <span class="comment"># torch.Size([1, 64, 1, 1])</span></span><br><span class="line">    <span class="built_in">print</span>(model_1(x).shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="SA模块"><a href="#SA模块" class="headerlink" title="SA模块"></a>SA模块</h2><p><img src="image%202.png" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SpatialAttentionModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param in_ch: 特征图输入通道数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SpatialAttentionModule, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">2</span>,out_channels=<span class="number">1</span>,kernel_size=<span class="number">7</span>,padding=<span class="number">3</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.act = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment"># [N,C,H,W] -&gt; [N,1,H,W]</span></span><br><span class="line">        avg = torch.mean(x,dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># [N,C,H,W] -&gt; [N,1,H,W]</span></span><br><span class="line">        <span class="built_in">max</span>,_ = torch.<span class="built_in">max</span>(x,dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># [N,1,H,W] -&gt; [N,2,H,W]</span></span><br><span class="line">        out = torch.concatenate((avg,<span class="built_in">max</span>),dim=<span class="number">1</span>)</span><br><span class="line">        out = self.act(self.conv(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.rand(<span class="number">1</span>,<span class="number">64</span>,<span class="number">16</span>,<span class="number">16</span>)</span><br><span class="line">    N,C,H,W = x.shape</span><br><span class="line">    model_2 = SpatialAttentionModule()</span><br><span class="line">    <span class="comment"># torch.Size([1, 1, 16, 16])</span></span><br><span class="line">    <span class="built_in">print</span>(model_2(x).shape)</span><br></pre></td></tr></table></figure><h2 id="总体代码"><a href="#总体代码" class="headerlink" title="总体代码"></a>总体代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChannelAttentionModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_ch,reduction_ratio</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param in_ch: 特征图输入通道数</span></span><br><span class="line"><span class="string">        :param reduction_ratio: 特征图通道数的减少比率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(ChannelAttentionModule, self).__init__()</span><br><span class="line">        self.max_pool = nn.AdaptiveMaxPool2d(<span class="number">1</span>)</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=in_ch,out_channels=in_ch // reduction_ratio,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=in_ch // reduction_ratio,out_channels=in_ch,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.act = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        max_part = self.fc2(self.fc1(self.max_pool(x)))</span><br><span class="line">        avg_part = self.fc2(self.fc1(self.avg_pool(x)))</span><br><span class="line">        out = max_part + avg_part</span><br><span class="line">        out = self.act(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpatialAttentionModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param in_ch: 特征图输入通道数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SpatialAttentionModule, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">2</span>,out_channels=<span class="number">1</span>,kernel_size=<span class="number">7</span>,padding=<span class="number">3</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.act = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment"># [N,C,H,W] -&gt; [N,1,H,W]</span></span><br><span class="line">        avg = torch.mean(x,dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># [N,C,H,W] -&gt; [N,1,H,W]</span></span><br><span class="line">        <span class="built_in">max</span>,_ = torch.<span class="built_in">max</span>(x,dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># [N,1,H,W] -&gt; [N,2,H,W]</span></span><br><span class="line">        out = torch.concatenate((avg,<span class="built_in">max</span>),dim=<span class="number">1</span>)</span><br><span class="line">        out = self.act(self.conv(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CBAM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_ch,reduction_ratio</span>):</span><br><span class="line">        <span class="built_in">super</span>(CBAM, self).__init__()</span><br><span class="line">        self.ca = ChannelAttentionModule(in_ch=in_ch,reduction_ratio=reduction_ratio)</span><br><span class="line">        self.sa = SpatialAttentionModule()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment"># [N,C,H,W] -&gt; [N,C,H,W]</span></span><br><span class="line">        ca_out = self.ca(x) * x</span><br><span class="line">        <span class="comment"># [N,C,H,W] -&gt; [N,C,H,W]</span></span><br><span class="line">        sa_out = self.sa(ca_out) * ca_out</span><br><span class="line">        <span class="keyword">return</span> sa_out</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.rand(<span class="number">1</span>,<span class="number">64</span>,<span class="number">16</span>,<span class="number">16</span>)</span><br><span class="line">    N,C,H,W = x.shape</span><br><span class="line">    model = CBAM(in_ch=C,reduction_ratio=<span class="number">8</span>)</span><br><span class="line">    <span class="built_in">print</span>(model(x).shape)</span><br><span class="line">    <span class="comment"># model_1 = ChannelAttentionModule(in_ch=C,reduction_ratio=8)</span></span><br><span class="line">    <span class="comment"># print(model_1(x).shape)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># model_2 = SpatialAttentionModule(in_ch=C)</span></span><br><span class="line">    <span class="comment"># print(model_2(x).shape)</span></span><br></pre></td></tr></table></figure><h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><h2 id="如何对特征图的通道方向做平均池化和最大池化"><a href="#如何对特征图的通道方向做平均池化和最大池化" class="headerlink" title="如何对特征图的通道方向做平均池化和最大池化"></a>如何对特征图的通道方向做平均池化和最大池化</h2><p>原因是 <code>nn.AdaptiveMaxPool2d(1)</code> 、 <code>nn.AdaptiveAvgPool2d(1)</code> 的作用都是对H、W方向进行池化</p><p>比如将[N,C,H,W]→[N,C,1,1]</p><h2 id="torch-max-报错"><a href="#torch-max-报错" class="headerlink" title="torch.max 报错"></a>torch.max 报错</h2><p><code>TypeError: expected Tensor as element 1 in argument 0, but got torch.return_types.max</code></p><p>感觉没有什么问题，于是检查对应的代码部分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [N,C,H,W] -&gt; [N,1,H,W]</span></span><br><span class="line">avg = torch.mean(x,dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># [N,C,H,W] -&gt; [N,1,H,W]</span></span><br><span class="line"><span class="built_in">max</span> = torch.<span class="built_in">max</span>(x,dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># [N,1,H,W] -&gt; [N,2,H,W]</span></span><br><span class="line">out = torch.concatenate((avg,<span class="built_in">max</span>),dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>打断点看看怎么回事，<code>torch.max</code>有两个返回值，第一个返回值是max的值，第二个返回值是max对应的下标</p><p><img src="image%203.png" alt="image.png"></p><h2 id="cbam模块的sa模块报错"><a href="#cbam模块的sa模块报错" class="headerlink" title="cbam模块的sa模块报错"></a>cbam模块的sa模块报错</h2><p><code>RuntimeError: The size of tensor a (10) must match the size of tensor b (16) at non-singleton dimension 3</code></p><p>查了一下发现是卷积层的padding没有设置，默认为0，这样就不能保证特征图的大小不变</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">2</span>,out_channels=<span class="number">1</span>,kernel_size=<span class="number">7</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">H_{out} = \left\lfloor\frac{H_{in}  + 2 \text{P} - \text K }{s} + 1\right\rfloor</script><p>观察该公式可以知道当$K = 2P+1$时能保证经过卷积后的特征图大小不变，即</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">K = <span class="number">7</span>,S = <span class="number">1</span>,P = <span class="number">3</span></span><br><span class="line">K = <span class="number">3</span>,S = <span class="number">1</span>,P = <span class="number">1</span></span><br><span class="line">K = <span class="number">1</span>,S = <span class="number">1</span>,P = <span class="number">0</span></span><br></pre></td></tr></table></figure><p>因此修改之后成功运行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">2</span>,out_channels=<span class="number">1</span>,kernel_size=<span class="number">7</span>,padding=<span class="number">3</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【implementation】SENets</title>
      <link href="/2024/08/14/%E3%80%90implementation-CVPR18%E3%80%91SENets/"/>
      <url>/2024/08/14/%E3%80%90implementation-CVPR18%E3%80%91SENets/</url>
      
        <content type="html"><![CDATA[<h1 id="CVPR-18-SENets"><a href="#CVPR-18-SENets" class="headerlink" title="[CVPR-18]SENets"></a>[CVPR-18]SENets</h1><h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><img src="/2024/08/14/%E3%80%90implementation-CVPR18%E3%80%91SENets/Untitled.png" class="" title="Untitled"><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SENets</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_ch,reduction_ratio</span>):</span><br><span class="line">        <span class="built_in">super</span>(SENets, self).__init__()</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            :param in_ch: 特征图输入通道数</span></span><br><span class="line"><span class="string">            :param reduction_ratio: 特征图通道数的减少比率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># [N,C,H,W] -&gt; [N,C,1,1]</span></span><br><span class="line">        self.Global_pooling = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            <span class="comment"># [N,C] -&gt; [N,C/r]</span></span><br><span class="line">            nn.Linear(in_features= in_ch,out_features=in_ch//reduction_ratio),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            <span class="comment"># [N,C/r] -&gt; [N,C]</span></span><br><span class="line">            nn.Linear(in_features=in_ch//reduction_ratio,out_features=in_ch)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            :param x: 输入特征图 [N,C,H,W]</span></span><br><span class="line"><span class="string">            :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        N,C,H,W = x.shape</span><br><span class="line">        <span class="comment"># [N,C,H,W] -&gt; [N,C,1,1] -&gt; [N,C]</span></span><br><span class="line">        out = self.Global_pooling(x).view(N,C)</span><br><span class="line">        <span class="comment"># [N,C] -&gt; [N,C]</span></span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="comment"># [N,C] -&gt; [N,C,1,1]</span></span><br><span class="line">        out = out[:,:,<span class="literal">None</span>,<span class="literal">None</span>]</span><br><span class="line">        <span class="comment"># [N,C,1,1] -&gt; [N,C,H,W]</span></span><br><span class="line">        out = x * out</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.randn(<span class="number">2</span>,<span class="number">64</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;input_shape&#123;&#125;&quot;</span>.<span class="built_in">format</span>(x.shape))</span><br><span class="line">    model = SENets(in_ch=<span class="number">64</span>,reduction_ratio=<span class="number">8</span>)</span><br><span class="line">    out = model(x)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output_shape&#123;&#125;&quot;</span>.<span class="built_in">format</span>(out.shape))</span><br></pre></td></tr></table></figure><h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><h2 id="Q：模型中的这条红线是什么意思，残差连接吗？"><a href="#Q：模型中的这条红线是什么意思，残差连接吗？" class="headerlink" title="Q：模型中的这条红线是什么意思，残差连接吗？"></a><strong>Q：模型中的这条红线是什么意思，残差连接吗？</strong></h2><p>生成的特征的经过$F_{sq}$、$F_{ex}$ 后得到的特征图为[N,C]，需要依赖原始输入U进行$F_{scale}$，所以并不是残差连接</p><h2 id="Q：池化层nn-AdaptiveAvgPool2d遇到的问题"><a href="#Q：池化层nn-AdaptiveAvgPool2d遇到的问题" class="headerlink" title="Q：池化层nn.AdaptiveAvgPool2d遇到的问题"></a>Q：池化层<code>nn.AdaptiveAvgPool2d</code>遇到的问题</h2><p>不太理解<code>nn.AdaptiveAvgPool2d(1)</code> 的含义，其实它的意思是将[N,C,H,W]→[N,C,1,1]</p><h2 id="Q：全连接层nn-Linear遇到的问题"><a href="#Q：全连接层nn-Linear遇到的问题" class="headerlink" title="Q：全连接层nn.Linear遇到的问题"></a><strong>Q：全连接层<code>nn.Linear</code>遇到的问题</strong></h2><p><code>nn.Linear</code> 接受的输入为[N,C]，因此输入全连接层之前需要把维度[N,C,1,1]压缩成[N,C]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [N,C,H,W] -&gt; [N,C,1,1] -&gt; [N,C]</span></span><br><span class="line">out = self.Global_pooling(x).view(N,C)</span><br></pre></td></tr></table></figure><h2 id="Q：论文涉及的一些细节"><a href="#Q：论文涉及的一些细节" class="headerlink" title="Q：论文涉及的一些细节"></a><strong>Q：论文涉及的一些细节</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [N,C,H,W] -&gt; [N,C,1,1]</span></span><br><span class="line">self.Global_pooling = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【implementation】ddpm</title>
      <link href="/2024/07/11/%E3%80%90implementation%E3%80%91ddpm/"/>
      <url>/2024/07/11/%E3%80%90implementation%E3%80%91ddpm/</url>
      
        <content type="html"><![CDATA[<h1 id="Dataset-CelebA"><a href="#Dataset-CelebA" class="headerlink" title="Dataset-CelebA"></a><font color = blue>Dataset-CelebA</font></h1><p>主要包括<strong>transforms数据增强</strong>、<strong>DatasetFolder加载无分类数据</strong></p><h2 id="transforms数据增强"><a href="#transforms数据增强" class="headerlink" title="transforms数据增强"></a>transforms数据增强</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">transforms = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">64</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">64</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), std=(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h2 id="DatasetFolder加载无分类数据"><a href="#DatasetFolder加载无分类数据" class="headerlink" title="DatasetFolder加载无分类数据"></a>DatasetFolder加载无分类数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据加载（无分类数据）</span></span><br><span class="line">celeba_set = DatasetFolder(</span><br><span class="line">    root=win_root,</span><br><span class="line">    loader=loader,</span><br><span class="line">    extensions=<span class="string">&quot;jpg&quot;</span>,</span><br><span class="line">    transform= transforms</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> DatasetFolder</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">win_root = <span class="string">&quot;F:\dataset\CelebA\\&quot;</span></span><br><span class="line">loader = <span class="keyword">lambda</span> x : Image.<span class="built_in">open</span>(x)</span><br><span class="line">extensions = <span class="string">&quot;jpg&quot;</span></span><br><span class="line"><span class="comment"># 数据增强</span></span><br><span class="line">transforms = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">64</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">64</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), std=(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 数据加载（无分类数据）</span></span><br><span class="line">celeba_set = DatasetFolder(</span><br><span class="line">    root=win_root,</span><br><span class="line">    loader=loader,</span><br><span class="line">    extensions=<span class="string">&quot;jpg&quot;</span>,</span><br><span class="line">    transform= transforms</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="comment"># if __name__ == &#x27;__main__&#x27;:</span></span><br><span class="line"><span class="comment">#     print(len(celeba_set.samples))</span></span><br></pre></td></tr></table></figure><h1 id="Scheduler-LinearScheduler"><a href="#Scheduler-LinearScheduler" class="headerlink" title="Scheduler-LinearScheduler"></a><font color = blue>Scheduler-LinearScheduler</font></h1><p>主要包括<strong>diffuison过程</strong>和<strong>denoise过程</strong>两部分</p><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>初始化的目的就是先把所需要的变量值计算好</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,time_step,beta_start,beta_end</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param time_step: 时间步t</span></span><br><span class="line"><span class="string">        :param beta_start: 调度器beta起始值</span></span><br><span class="line"><span class="string">        :param beta_end: 调度器beta结束值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    self.time_step = time_step</span><br><span class="line">    self.beta_start = beta_start</span><br><span class="line">    self.beta_end = beta_end</span><br><span class="line">    self.beta_t = torch.linspace(beta_start,beta_end,time_step)</span><br><span class="line">    self.alpha_t = <span class="number">1.</span> - self.beta_t</span><br><span class="line">    self.alpha_t_bar = torch.cumprod(self.alpha_t,dim=<span class="number">0</span>)</span><br><span class="line">    self.one_minus_alpha_t_bar = <span class="number">1.</span> - self.alpha_t_bar</span><br><span class="line">    self.one_minus_alpha_t = <span class="number">1.</span> - self.alpha_t</span><br><span class="line">    self.sqrt_alpha_t_bar = torch.sqrt(self.alpha_t_bar)</span><br><span class="line">    self.sqrt_one_minus_alpha_t_bar = torch.sqrt(self.one_minus_alpha_t_bar)</span><br></pre></td></tr></table></figure><h2 id="diffuison过程"><a href="#diffuison过程" class="headerlink" title="diffuison过程"></a>diffuison过程</h2><p>diffusion过程主要计算以下公式</p><script type="math/tex; mode=display">x_{t} = \sqrt{ \bar \alpha_{t}}x_0 + \sqrt{1-\bar \alpha_{t}}\epsilon</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">diffusion</span>(<span class="params">self,x_0,t,noise</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        扩散模型的加噪过程</span></span><br><span class="line"><span class="string">        :param x_0: 输入源图像 x_0</span></span><br><span class="line"><span class="string">        :param t: 输入第t个时间步</span></span><br><span class="line"><span class="string">        :param noise: 对x_0加入的带噪图像</span></span><br><span class="line"><span class="string">        :return: 返回第t个时间刻的带噪图像 x_t</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    sqrt_alpha_t_bar = self.sqrt_alpha_t_bar[t]</span><br><span class="line">    sqrt_one_minus_alpha_t_bar = self.sqrt_one_minus_alpha_t_bar[t]</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_0.shape) - <span class="number">1</span>):</span><br><span class="line">    sqrt_alpha_t_bar = sqrt_alpha_t_bar.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_0.shape) - <span class="number">1</span>):</span><br><span class="line">    sqrt_one_minus_alpha_t_bar = sqrt_one_minus_alpha_t_bar.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">    x_t = sqrt_alpha_t_bar * x_0 + sqrt_one_minus_alpha_t_bar * noise</span><br><span class="line">    <span class="keyword">return</span> x_t</span><br></pre></td></tr></table></figure><h2 id="denoise过程"><a href="#denoise过程" class="headerlink" title="denoise过程"></a>denoise过程</h2><p>denoise过程主要计算以下公式</p><script type="math/tex; mode=display">x_{t-1}= \frac{1}{\sqrt {\alpha_t }}(x_t - \frac{1 - \alpha_t }{\sqrt{1-{\bar \alpha_t }} }\epsilon_t) + \sigma_tz</script><script type="math/tex; mode=display">{\mu} = \frac{1}{\sqrt {\alpha_t }}(x_t - \frac{1 - \alpha_t }{\sqrt{1-{\bar \alpha_t }} }\epsilon_t)</script><script type="math/tex; mode=display">\sigma = \frac{1-\bar\alpha _{t-1}  }{1 - \bar \alpha _t} \beta _t</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">denoise</span>(<span class="params">self,x_t,t,noise</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        扩散模型的去噪过程</span></span><br><span class="line"><span class="string">        :param x_t: 时间步t所对应的带噪图像</span></span><br><span class="line"><span class="string">        :param t: 时间步t</span></span><br><span class="line"><span class="string">        :param noise: 需要去除的噪声图像</span></span><br><span class="line"><span class="string">        :return: 去噪后的图像x_&#123;t-1&#125;，当t=0时直接返回均值，否则返回均值+方差</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    mean = (x_t - ((self.beta_t[t]) * noise / self.one_minus_alpha_t_bar[t])) / self.sqrt_alpha_t_bar[t]</span><br><span class="line">    variance = (<span class="number">1.</span> - self.alpha_t_bar[t-<span class="number">1</span>]) * (<span class="number">1.</span> - self.alpha_t[t]) / (self.one_minus_alpha_t_bar[t])</span><br><span class="line">    z = torch.randn(x_t.shape)</span><br><span class="line">    <span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">return</span> mean</span><br><span class="line">    <span class="keyword">return</span> mean + variance ** <span class="number">0.5</span> * z</span><br></pre></td></tr></table></figure><h2 id="完整代码-1"><a href="#完整代码-1" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearScheduler</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        扩散模型的线性调度器，主要分为两个部分</span></span><br><span class="line"><span class="string">        1、扩散过程，用于获得任意时间刻的带噪图像</span></span><br><span class="line"><span class="string">        2、去噪过程，基于前一个时刻图像去噪以获得去噪图像</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,time_step,beta_start,beta_end</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            :param time_step: 时间步t</span></span><br><span class="line"><span class="string">            :param beta_start: 调度器beta起始值</span></span><br><span class="line"><span class="string">            :param beta_end: 调度器beta结束值</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.time_step = time_step</span><br><span class="line">        self.beta_start = beta_start</span><br><span class="line">        self.beta_end = beta_end</span><br><span class="line">        self.beta_t = torch.linspace(beta_start,beta_end,time_step)</span><br><span class="line">        self.alpha_t = <span class="number">1.</span> - self.beta_t</span><br><span class="line">        self.alpha_t_bar = torch.cumprod(self.alpha_t,dim=<span class="number">0</span>)</span><br><span class="line">        self.one_minus_alpha_t_bar = <span class="number">1.</span> - self.alpha_t_bar</span><br><span class="line">        self.one_minus_alpha_t = <span class="number">1.</span> - self.alpha_t</span><br><span class="line">        self.sqrt_alpha_t_bar = torch.sqrt(self.alpha_t_bar)</span><br><span class="line">        self.sqrt_one_minus_alpha_t_bar = torch.sqrt(self.one_minus_alpha_t_bar)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">diffusion</span>(<span class="params">self,x_0,t,noise</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            扩散模型的加噪过程</span></span><br><span class="line"><span class="string">            :param x_0: 输入源图像 x_0</span></span><br><span class="line"><span class="string">            :param t: 输入第t个时间步</span></span><br><span class="line"><span class="string">            :param noise: 对x_0加入的带噪图像</span></span><br><span class="line"><span class="string">            :return: 返回第t个时间刻的带噪图像 x_t</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sqrt_alpha_t_bar = self.sqrt_alpha_t_bar[t]</span><br><span class="line">        sqrt_one_minus_alpha_t_bar = self.sqrt_one_minus_alpha_t_bar[t]</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_0.shape) - <span class="number">1</span>):</span><br><span class="line">            sqrt_alpha_t_bar = sqrt_alpha_t_bar.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_0.shape) - <span class="number">1</span>):</span><br><span class="line">            sqrt_one_minus_alpha_t_bar = sqrt_one_minus_alpha_t_bar.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">        x_t = sqrt_alpha_t_bar * x_0 + sqrt_one_minus_alpha_t_bar * noise</span><br><span class="line">        <span class="keyword">return</span> x_t</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">denoise</span>(<span class="params">self,x_t,t,noise</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            扩散模型的去噪过程</span></span><br><span class="line"><span class="string">            :param x_t: 时间步t所对应的带噪图像</span></span><br><span class="line"><span class="string">            :param t: 时间步t</span></span><br><span class="line"><span class="string">            :param noise: 需要去除的噪声图像</span></span><br><span class="line"><span class="string">            :return: 去噪后的图像x_&#123;t-1&#125;，当t=0时直接返回均值，否则返回均值+方差</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        mean = (x_t - ((self.beta_t[t]) * noise / self.one_minus_alpha_t_bar[t])) / self.sqrt_alpha_t_bar[t]</span><br><span class="line">        variance = (<span class="number">1.</span> - self.alpha_t_bar[t-<span class="number">1</span>]) * (<span class="number">1.</span> - self.alpha_t[t]) / (self.one_minus_alpha_t_bar[t])</span><br><span class="line">        z = torch.randn(x_t.shape)</span><br><span class="line">        <span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> mean</span><br><span class="line">        <span class="keyword">return</span> mean + variance ** <span class="number">0.5</span> * z</span><br><span class="line"></span><br><span class="line"><span class="comment"># if __name__ == &#x27;__main__&#x27;:</span></span><br><span class="line"><span class="comment">#     x = torch.rand(2,3,64,64)</span></span><br><span class="line"><span class="comment">#     noise = torch.rand(1, 3, 64, 64)</span></span><br><span class="line"><span class="comment">#     Scheduler = LinearScheduler(1000,0.1,0.9)</span></span><br><span class="line"><span class="comment">#     t = torch.randint(0, 1000, (x.shape[0],))</span></span><br><span class="line"><span class="comment">#     tt = torch.as_tensor(0).unsqueeze(0)</span></span><br><span class="line"><span class="comment">#     add_noise = Scheduler.diffusion(x,t,noise)</span></span><br><span class="line"><span class="comment">#     print(add_noise.shape)</span></span><br><span class="line"><span class="comment">#     denoise_noise = Scheduler.denoise(x,tt,noise)</span></span><br><span class="line"><span class="comment">#     print(denoise_noise.shape)</span></span><br></pre></td></tr></table></figure><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><h3 id="刚开始在写扩散过程时，代码报错维度不匹配，代码如下图所示"><a href="#刚开始在写扩散过程时，代码报错维度不匹配，代码如下图所示" class="headerlink" title="刚开始在写扩散过程时，代码报错维度不匹配，代码如下图所示"></a>刚开始在写扩散过程时，代码报错维度不匹配，代码如下图所示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">diffusion</span>(<span class="params">self,x_0,t,noise</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        扩散模型的加噪过程</span></span><br><span class="line"><span class="string">        :param x_0: 输入源图像 x_0</span></span><br><span class="line"><span class="string">        :param t: 输入第t个时间步</span></span><br><span class="line"><span class="string">        :param noise: 对x_0加入的带噪图像</span></span><br><span class="line"><span class="string">        :return: 返回第t个时间刻的带噪图像 x_t</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x_t = self.sqrt_alpha_t_bar[t] * x_0 + self.sqrt_one_minus_alpha_t_bar[t] * noise</span><br><span class="line">    <span class="keyword">return</span> x_t</span><br></pre></td></tr></table></figure><p>此时，alpha_t的维度为(N,)，x_0的维度为(N,3,64,64)，前者没有1,2,3维度，自然不能进行广播，因此需要将alpha_t的维度升维成(N,1,1,1)以匹配x_0，才能进行广播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">diffusion</span>(<span class="params">self,x_0,t,noise</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        扩散模型的加噪过程</span></span><br><span class="line"><span class="string">        :param x_0: 输入源图像 x_0</span></span><br><span class="line"><span class="string">        :param t: 输入第t个时间步</span></span><br><span class="line"><span class="string">        :param noise: 对x_0加入的带噪图像</span></span><br><span class="line"><span class="string">        :return: 返回第t个时间刻的带噪图像 x_t</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">   sqrt_alpha_t_bar = self.sqrt_alpha_t_bar[t]</span><br><span class="line">   sqrt_one_minus_alpha_t_bar = self.sqrt_one_minus_alpha_t_bar[t]</span><br><span class="line">   <span class="comment"># 改进部分,unsqueeze在最后一个维度升维</span></span><br><span class="line">   <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_0.shape) - <span class="number">1</span>):</span><br><span class="line">      sqrt_alpha_t_bar = sqrt_alpha_t_bar.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">   <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_0.shape) - <span class="number">1</span>):</span><br><span class="line">      sqrt_one_minus_alpha_t_bar = sqrt_one_minus_alpha_t_bar.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">    x_t = sqrt_alpha_t_bar * x_0 + sqrt_one_minus_alpha_t_bar * noise</span><br><span class="line">    <span class="keyword">return</span> x_t</span><br></pre></td></tr></table></figure><h1 id="Model-UNet"><a href="#Model-UNet" class="headerlink" title="Model-UNet"></a><font color = blue>Model-UNet</font></h1><p>主要包括<strong>time_embedding</strong>、<strong>downblock</strong>、<strong>midblock</strong>、<strong>upblock</strong></p><h2 id="time-embeding"><a href="#time-embeding" class="headerlink" title="time_embeding"></a>time_embeding</h2><p>该模块的作用是将时间处理成一种位置信息，让扩散模型学习这种时间t与噪声的关系，采用的位置编码方式是和transformer一致的正余弦（sinusoidal）曲线编码，主要参考以下公式</p><script type="math/tex; mode=display">PE_{(pos, 2i)} = \sin( \frac {pos}{10000^{2i/d_{model}}})</script><script type="math/tex; mode=display">PE_{(pos, 2i+1)} = \cos( \frac {pos}{10000^ {2i/d_{model}}})</script><script type="math/tex; mode=display">\\i \in [0,d_{model/2})</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_time_embedding</span>(<span class="params">pos,d_model</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos: 第pos个位置,标量</span></span><br><span class="line"><span class="string">        :param d_model: 位置编码对应的嵌入维度</span></span><br><span class="line"><span class="string">        :return: 第pos个位置的位置编码</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    factor = <span class="number">10000</span> ** (</span><br><span class="line">        (torch.arange(<span class="number">0</span>,d_model//<span class="number">2</span>)) / (d_model//<span class="number">2</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># pos -&gt; [N,] -&gt; [N,1] -&gt; [N,d_model//2]</span></span><br><span class="line">    pos = pos.unsqueeze(-<span class="number">1</span>).repeat(<span class="number">1</span>, d_model // <span class="number">2</span>)</span><br><span class="line">    angle = pos / factor</span><br><span class="line">    pos_code = torch.cat([torch.sin(angle),torch.cos(angle)],dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> pos_code</span><br><span class="line"></span><br><span class="line"><span class="comment"># if __name__ == &#x27;__main__&#x27;:</span></span><br><span class="line"><span class="comment">#     x = torch.randn(1,3,64,64)</span></span><br><span class="line"><span class="comment">#     tt = torch.randint(1, 2, (x.shape[0],))</span></span><br><span class="line"><span class="comment">#     y = get_time_embedding(tt,32)</span></span><br></pre></td></tr></table></figure><h2 id="downblock"><a href="#downblock" class="headerlink" title="downblock"></a>downblock</h2><p>主要分为<strong>Resnet</strong>和<strong>SA</strong>模块，最终实现将输入【N,C1,H,W】 —- 【N,C2,H/2,W/2】 —- 【N,C3,H/4,W/4】—- 【N,C4,H/8,W/8】</p><p><style>.lfuegeqbclkr{zoom: 33%;}</style><img src="/2024/07/11/%E3%80%90implementation%E3%80%91ddpm/image-20240716160759224.png" class="lfuegeqbclkr" alt="image-20240716160759224"></p><h3 id="Resnet模块"><a href="#Resnet模块" class="headerlink" title="Resnet模块"></a>Resnet模块</h3><p>主要分为<strong>first_conv</strong>和<strong>second_conv</strong>两层卷积，采用<code>k=3,s=1,p=1</code>的卷积使得卷积操作只改变通道数，而不改变图像尺寸大小</p><p>其次<strong>time_emb</strong>会和<strong>first_conv</strong>结合</p><p>最终张量【N,C1,H,W】经过Resnet模块得到输出【N,C2,H,W】，其中C2 = 2 * C1（比如C1=32，C2为64）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>:</span><br><span class="line">    self.first_conv = nn.Sequential(</span><br><span class="line">        nn.GroupNorm(num_groups=<span class="number">8</span>, num_channels=in_ch),</span><br><span class="line">        nn.SiLU(),</span><br><span class="line">        nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    self.second_conv = nn.Sequential(</span><br><span class="line">        nn.GroupNorm(num_groups=<span class="number">8</span>, num_channels=out_ch),</span><br><span class="line">        nn.SiLU(),</span><br><span class="line">        nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    self.time_proj = nn.Sequential(</span><br><span class="line">        nn.SiLU(),</span><br><span class="line">        nn.Linear(in_features=t_emb,out_features=out_ch)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    self.res_conv = nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=<span class="number">1</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>:</span><br><span class="line">    <span class="comment">#ResNet</span></span><br><span class="line">    out = self.first_conv(<span class="built_in">input</span>)</span><br><span class="line">    out = self.time_proj(t_emb).unsqueeze(-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>) + out</span><br><span class="line">    out = self.second_conv(out)</span><br><span class="line">    out = out + self.res_conv(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure><h3 id="SA模块"><a href="#SA模块" class="headerlink" title="SA模块"></a>SA模块</h3><p>张量【N,C2,H,W】经过SA模块得到输出【N,C2,H,W】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>:</span><br><span class="line">    self.self_attn_norm = nn.GroupNorm(num_groups=<span class="number">8</span>,num_channels=out_ch)</span><br><span class="line">    self.self_attn = nn.MultiheadAttention(embed_dim=out_ch, num_heads=<span class="number">8</span>,batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>:</span><br><span class="line">    <span class="comment">#SA</span></span><br><span class="line">    batch,ch,h,w = out.shape</span><br><span class="line">    attn_in = out</span><br><span class="line">    attn_in = attn_in.reshape(batch,ch,h*w)</span><br><span class="line">    attn_in = self.self_attn_norm(attn_in)</span><br><span class="line">    attn_in = attn_in.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    attn_out,_ = self.self_attn(attn_in,attn_in,attn_in)</span><br><span class="line">    attn_out = attn_out.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    attn_out = attn_out.reshape(batch,ch,h,w)</span><br><span class="line">    out = out + attn_out</span><br><span class="line">    out = self.down_conv(out)</span><br></pre></td></tr></table></figure><h3 id="如何理解多头注意力机制nn-MultiheadAttention的使用"><a href="#如何理解多头注意力机制nn-MultiheadAttention的使用" class="headerlink" title="如何理解多头注意力机制nn.MultiheadAttention的使用"></a>如何理解多头注意力机制<code>nn.MultiheadAttention</code>的使用</h3><p>QKV需要满足【N,L,E】的维度输入，我们的上一步的输入为【N,C2,H,W】，因此需要reshape成【N,C2,HW】再transpose成【N,HW,C】后才能作为输入数据</p><p><img src="image-20240712230237309.png" alt="image-20240712230237309"></p><p><style>.ertfltycfwml{zoom:50%;}</style><img src="/2024/07/11/%E3%80%90implementation%E3%80%91ddpm/image-20240712230212731.png" class="ertfltycfwml" alt="image-20240712230212731"></p><h2 id="midblock"><a href="#midblock" class="headerlink" title="midblock"></a>midblock</h2><p>同理包含<strong>Restnet模块</strong>和<strong>SA模块</strong>，最终实现将输入【N,C4,H/8,W/8】—-【N,C3,H/8,W/8】</p><p><style>.dqivmjxdflob{zoom: 33%;}</style><img src="/2024/07/11/%E3%80%90implementation%E3%80%91ddpm/image-20240716160841252.png" class="dqivmjxdflob" alt="image-20240716160841252"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_ch,out_ch,t_emb,num_head=<span class="number">8</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            :param in_ch: 输入通道数</span></span><br><span class="line"><span class="string">            :param out_ch: 输出通道数</span></span><br><span class="line"><span class="string">            :param t_emb: 时间位置编码嵌入</span></span><br><span class="line"><span class="string">            :param num_head: 注意力机制多头数量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">super</span>(MidBlock, self).__init__()</span><br><span class="line">    self.in_ch = in_ch</span><br><span class="line">    self.out_ch = out_ch</span><br><span class="line">    self.t_emb = t_emb</span><br><span class="line"></span><br><span class="line">    self.first_conv = nn.Sequential(</span><br><span class="line">        nn.GroupNorm(num_groups=<span class="number">8</span>,num_channels=in_ch),</span><br><span class="line">        nn.SiLU(),</span><br><span class="line">        nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    self.second_conv = nn.Sequential(</span><br><span class="line">        nn.GroupNorm(num_groups=<span class="number">8</span>,num_channels=out_ch),</span><br><span class="line">        nn.SiLU(),</span><br><span class="line">        nn.Conv2d(in_channels=out_ch,out_channels=out_ch,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    self.t_proj= nn.Sequential(</span><br><span class="line">        nn.SiLU(),</span><br><span class="line">        nn.Linear(in_features=t_emb,out_features=out_ch)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    self.resnet_conv = nn.Sequential(</span><br><span class="line">        nn.SiLU(),</span><br><span class="line">        nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=<span class="number">1</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    self.third_conv = nn.Sequential(</span><br><span class="line">        nn.SiLU(),</span><br><span class="line">        nn.Conv2d(in_channels=out_ch,out_channels=out_ch,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    self.self_attn_norm = nn.GroupNorm(num_groups=<span class="number">8</span>,num_channels=out_ch)</span><br><span class="line">    self.self_attn = nn.MultiheadAttention(embed_dim=out_ch,num_heads=num_head,batch_first=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span>,t</span>):</span><br><span class="line">    <span class="comment">#Resnet</span></span><br><span class="line">    out = self.first_conv(<span class="built_in">input</span>)</span><br><span class="line">    out = out + self.t_proj(t).unsqueeze(-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">    out = self.second_conv(out)</span><br><span class="line">    out = out + self.resnet_conv(<span class="built_in">input</span>)</span><br><span class="line">    <span class="comment">#SA</span></span><br><span class="line">    out_copy_first = out</span><br><span class="line">    batch,ch,h,w = out.shape</span><br><span class="line">    out = out.reshape(batch,ch,h*w)</span><br><span class="line">    in_attn = self.self_attn_norm(out)</span><br><span class="line">    in_attn = in_attn.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    out_attn,_ = self.self_attn(in_attn,in_attn,in_attn)</span><br><span class="line">    out = out_attn.transpose(<span class="number">1</span>,<span class="number">2</span>).reshape(batch,ch,h,w)</span><br><span class="line">    out = out + out_copy_first</span><br><span class="line">    <span class="comment">#Resnet</span></span><br><span class="line">    out_copy_second = out</span><br><span class="line">    out = self.third_conv(out)</span><br><span class="line">    out = self.third_conv(out)</span><br><span class="line">    out = out_copy_second + out</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h2 id="upblock"><a href="#upblock" class="headerlink" title="upblock"></a>upblock</h2><p>主要包含<strong>Restnet模块</strong>，最终实现将输入【N,C3,H/8,W/8】 —-【N,C2,H/4,W/4】—- 【N,C1,H/2,W/2】—- 【N,16,H,W】—- 【N,3,H,W】</p><p><style>.mofopvwfpxpl{zoom: 50%;}</style><img src="/2024/07/11/%E3%80%90implementation%E3%80%91ddpm/image-20240716160912258.png" class="mofopvwfpxpl" alt="image-20240716160912258"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_ch,out_ch</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            :param in_ch: 特征图输入通道数</span></span><br><span class="line"><span class="string">            :param out_ch: 特征图输出通道数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">super</span>(UpBlock, self).__init__()</span><br><span class="line">    self.in_ch = in_ch</span><br><span class="line">    self.out_ch = out_ch</span><br><span class="line">    self.transpose_conv = nn.Sequential(</span><br><span class="line">        nn.ConvTranspose2d(in_channels=in_ch,out_channels=in_ch,kernel_size=<span class="number">4</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    self.upsample_conv = nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels=in_ch*<span class="number">2</span>,out_channels=out_ch,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span>,down_input</span>):</span><br><span class="line">    out = self.transpose_conv(<span class="built_in">input</span>)</span><br><span class="line">    out = torch.cat([out,down_input],dim=<span class="number">1</span>)</span><br><span class="line">    out = self.upsample_conv(out)</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line">        </span><br></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    x = torch.randn(1,32,64,64)</span><br><span class="line">    t = get_time_embedding(torch.as_tensor(10),128)</span><br><span class="line">    down_1 = DownBlock(in_ch=32,out_ch=64,down_sample=True,t_emb=128)</span><br><span class="line">    down_2 = DownBlock(in_ch=64,out_ch=128,down_sample=True,t_emb=128)</span><br><span class="line">    mid = MidBlock(in_ch=128,out_ch=64,t_emb=128)</span><br><span class="line">    up = UpBlock(in_ch=64,out_ch=32)</span><br><span class="line"></span><br><span class="line">    down_1_ans = down_1(x,t)</span><br><span class="line">    down_2_ans = down_2(down_1_ans,t)</span><br><span class="line">    print(&quot;一次down:&#123;&#125;&quot;.format(down_1_ans.shape))</span><br><span class="line">    print(&quot;二次down:&#123;&#125;&quot;.format(down_2_ans.shape))</span><br><span class="line">    mid_ans = mid(down_2_ans,t)</span><br><span class="line">    print(&quot;一次mid:&#123;&#125;&quot;.format(mid_ans.shape))</span><br><span class="line">    out_ans = up(mid_ans,down_1_ans)</span><br><span class="line">    print(&quot;一次up:&#123;&#125;&quot;.format(out_ans.shape))</span><br></pre></td></tr></table></figure><h2 id="完整代码-2"><a href="#完整代码-2" class="headerlink" title="完整代码"></a>完整代码</h2>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Docker部署后端及配置集群</title>
      <link href="/2024/04/12/Docker%E9%83%A8%E7%BD%B2%E5%90%8E%E7%AB%AF%E5%8F%8A%E9%85%8D%E7%BD%AE%E9%9B%86%E7%BE%A4/"/>
      <url>/2024/04/12/Docker%E9%83%A8%E7%BD%B2%E5%90%8E%E7%AB%AF%E5%8F%8A%E9%85%8D%E7%BD%AE%E9%9B%86%E7%BE%A4/</url>
      
        <content type="html"><![CDATA[<h1 id="Docker部署后端及配置集群"><a href="#Docker部署后端及配置集群" class="headerlink" title="Docker部署后端及配置集群"></a>Docker部署后端及配置集群</h1><h2 id="1、后端项目打包"><a href="#1、后端项目打包" class="headerlink" title="1、后端项目打包"></a>1、后端项目打包</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install -Dmaven.test.skip=true</span><br></pre></td></tr></table></figure><h2 id="2、安装java镜像"><a href="#2、安装java镜像" class="headerlink" title="2、安装java镜像"></a>2、安装java镜像</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull openjdk</span><br></pre></td></tr></table></figure><h2 id="3、创建并启动java容器"><a href="#3、创建并启动java容器" class="headerlink" title="3、创建并启动java容器"></a>3、创建并启动java容器</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#创建数据卷，上传JAR文件</span><br><span class="line">docker volume create j1</span><br><span class="line">#启动容器</span><br><span class="line">docker run -it -d --name j1 -v j1:/home/soft --net=host openjdk</span><br><span class="line">#进入j1容器</span><br><span class="line">docker exec -it j1 bash</span><br><span class="line">#启动Java项目</span><br><span class="line">nohup java -jar /home/soft/你的jar包名字.jar &gt;&gt; home/soft/java.log 2&gt;&amp;1 &amp;</span><br><span class="line">#查看日志</span><br><span class="line">tail -100f /home/soft/java.log</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><style>.jmhmxyamfhjq{zoom:50%;}</style><img src="/2024/04/12/Docker%E9%83%A8%E7%BD%B2%E5%90%8E%E7%AB%AF%E5%8F%8A%E9%85%8D%E7%BD%AE%E9%9B%86%E7%BE%A4/image-20240412135754999.png" class="jmhmxyamfhjq" alt="image-20240412135754999"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#同理部署两个后端节点，记得修改好配置文件端口号</span><br><span class="line">docker volume create j2</span><br><span class="line">docker run -it -d --name j2 -v j2:/home/soft --net=host openjdk</span><br><span class="line">docker exec -it j2 bash</span><br><span class="line">nohup java -jar /home/soft/你的jar包名字.jar &gt;&gt; home/soft/java.log 2&gt;&amp;1 &amp;</span><br><span class="line">tail -100f /home/soft/java.log</span><br><span class="line"></span><br><span class="line">docker volume create j3</span><br><span class="line">docker run -it -d --name j3 -v j3:/home/soft --net=host openjdk</span><br><span class="line">docker exec -it j3 bash</span><br><span class="line">nohup java -jar /home/soft/你的jar包名字.jar &gt;&gt; home/soft/java.log 2&gt;&amp;1 &amp;</span><br><span class="line">tail -100f /home/soft/java.log</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="4、安装nginx镜像"><a href="#4、安装nginx镜像" class="headerlink" title="4、安装nginx镜像"></a>4、安装nginx镜像</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull nginx:1.25.1</span><br></pre></td></tr></table></figure><h2 id="5、对应目录创建nginx配置文件"><a href="#5、对应目录创建nginx配置文件" class="headerlink" title="5、对应目录创建nginx配置文件"></a>5、对应目录创建nginx配置文件</h2><p><style>.hhdlrvfofhbv{zoom:50%;}</style><img src="/2024/04/12/Docker%E9%83%A8%E7%BD%B2%E5%90%8E%E7%AB%AF%E5%8F%8A%E9%85%8D%E7%BD%AE%E9%9B%86%E7%BE%A4/image-20240412193546678.png" class="hhdlrvfofhbv" alt="image-20240412193546678"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">user  nginx;</span><br><span class="line">worker_processes  1;</span><br><span class="line">error_log  /var/log/nginx/error.log warn;</span><br><span class="line">pid        /var/run/nginx.pid;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    include       /etc/nginx/mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line"></span><br><span class="line">    log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;</span><br><span class="line">                      &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;</span><br><span class="line">                      &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;</span><br><span class="line"></span><br><span class="line">    access_log  /var/log/nginx/access.log  main;</span><br><span class="line"></span><br><span class="line">    sendfile        on;</span><br><span class="line">    #tcp_nopush     on;</span><br><span class="line"></span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line"></span><br><span class="line">    #gzip  on;</span><br><span class="line"></span><br><span class="line">proxy_redirect          off;</span><br><span class="line">proxy_set_header        Host $host;</span><br><span class="line">proxy_set_header        X-Real-IP $remote_addr;</span><br><span class="line">proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">client_max_body_size    3072m;</span><br><span class="line">client_body_buffer_size   128k;</span><br><span class="line">proxy_connect_timeout   600s;</span><br><span class="line">proxy_send_timeout      600s;</span><br><span class="line">proxy_read_timeout      600s;</span><br><span class="line">proxy_buffer_size        4k;</span><br><span class="line">proxy_buffers           4 32k;</span><br><span class="line">proxy_busy_buffers_size  64k;</span><br><span class="line">proxy_temp_file_write_size 64k;</span><br><span class="line"></span><br><span class="line">upstream tomcat &#123;</span><br><span class="line">server 你的ip:8081;</span><br><span class="line">server 你的ip:8082;</span><br><span class="line">server 你的ip:8083;</span><br><span class="line">&#125;</span><br><span class="line">server &#123;</span><br><span class="line">        listen       8888;</span><br><span class="line">        server_name  你的ip;</span><br><span class="line">        location / &#123;</span><br><span class="line">            proxy_pass   http://tomcat;</span><br><span class="line">            index  index.html index.htm;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>指的是前端请求后端8888端口（根据自己的需求修改端口号），nginx根据负载均衡策略转发给8081、8082、8083（根据自己的需求修改端口号）</p><h2 id="6、创建并启动nginx容器"><a href="#6、创建并启动nginx容器" class="headerlink" title="6、创建并启动nginx容器"></a>6、创建并启动nginx容器</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -d --name n1 -v /home/n1/nginx.conf:/etc/nginx/nginx.conf --net=host --privileged nginx:1.25.1</span><br></pre></td></tr></table></figure><h2 id="7、整体结构"><a href="#7、整体结构" class="headerlink" title="7、整体结构"></a>7、整体结构</h2><p><style>.oydygwyewtxp{zoom: 33%;}</style><img src="/2024/04/12/Docker%E9%83%A8%E7%BD%B2%E5%90%8E%E7%AB%AF%E5%8F%8A%E9%85%8D%E7%BD%AE%E9%9B%86%E7%BE%A4/image-20240412195054467.png" class="oydygwyewtxp" alt="image-20240412195054467"></p><h2 id="8、检查"><a href="#8、检查" class="headerlink" title="8、检查"></a>8、检查</h2><p>检查云服务器端口开放情况</p><p>检查java.log日志运行情况</p>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Docker配置RocketMQ</title>
      <link href="/2024/04/02/RocketMQ/"/>
      <url>/2024/04/02/RocketMQ/</url>
      
        <content type="html"><![CDATA[<h1 id="Docker配置RocketMQ"><a href="#Docker配置RocketMQ" class="headerlink" title="Docker配置RocketMQ"></a>Docker配置RocketMQ</h1><p><strong>1、拉取RocketMQ镜像</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull foxiswho/rocketmq:broker-4.5.1</span><br></pre></td></tr></table></figure><p><strong>2、启动注册中心Nameserver</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 9876:9876 --name rmqserver foxiswho/rocketmq:server-4.5.1</span><br></pre></td></tr></table></figure><p><strong>3、创建RocketMQ配置文件</strong></p><p><style>.gnuinscepddr{zoom: 50%;}</style><img src="/2024/04/02/RocketMQ/image-20240411235840559.png" class="gnuinscepddr" alt="image-20240411235840559"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//或者本地创建后上传到云主机上，路径可以根据自己设定</span><br><span class="line">vim /usr/local/roketmq/conf/broker.conf</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">brokerIP1 = 填写你的云主机ip</span><br><span class="line">brokerClusterName = DefaultCluster</span><br><span class="line">brokerName = broker-a</span><br><span class="line">brokerId = 0</span><br><span class="line">deleteWhen = 04</span><br><span class="line">fileReservedTime = 48</span><br><span class="line">brokerRole = ASYNC_MASTER</span><br><span class="line">flushDiskType = ASYNC_FLUSH</span><br><span class="line">enablePropertyFilter=true</span><br></pre></td></tr></table></figure><p><strong>4、启动RocketMQ</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 10911:10911 -p 10909:10909 --name rmqbroker --link rmqserver:namesrv -e &quot;NAMESRV_ADDR=namesrv:9876&quot; -e &quot;JAVA_OPTS=-Duser.home=/opt&quot; -e &quot;JAVA_OPT_EXT=-server -Xms128m -Xmx128m&quot; -v  /usr/local/rocketmq/conf/broker.conf:/etc/rocketmq/broker.conf foxiswho/rocketmq:broker-4.5.1</span><br></pre></td></tr></table></figure><p><strong>5、拉取RocketMQ-dashboard镜像</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull pangliang/rocketmq-console-ng</span><br></pre></td></tr></table></figure><p><strong>6、启动dashboard</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name rmqconsole -p 8180:8080 --link rmqserver:namesrv -e &quot;JAVA_OPTS=-Drocketmq.namesrv.addr=namesrv:9876 -Dcom.rocketmq.sendMessageWithVIPChannel=false&quot;  -t pangliang/rocketmq-console-ng</span><br></pre></td></tr></table></figure><p><strong>7、dashboard中添加topic</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://xxx.xxx.xxx.xxx:8180</span><br></pre></td></tr></table></figure><p><strong>8、检查</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//检查服务运行状况</span><br><span class="line">docker ps -a</span><br></pre></td></tr></table></figure><p><img src="image-20240412000329429.png" alt="image-20240412000329429"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//检查服务运行是否成功、依次检查Nameserver、RocketMQ、RocketMQ-dashboard（暂不演示）</span><br><span class="line">docker logs -CONTAINER ID</span><br></pre></td></tr></table></figure><p><img src="image-20240412000517073.png" alt="image-20240412000517073"></p><p><img src="image-20240412000735422.png" alt="image-20240412000735422"></p><p><style>.ubsbyktezyad{zoom:33%;}</style><img src="/2024/04/02/RocketMQ/image-20240412001315222.png" class="ubsbyktezyad" alt="image-20240412001315222"></p><p><style>.rolgnboovobr{zoom: 33%;}</style><img src="/2024/04/02/RocketMQ/image-20240412001331680.png" class="rolgnboovobr" alt="image-20240412001331680"></p><p><style>.edkeccovawjg{zoom:33%;}</style><img src="/2024/04/02/RocketMQ/image-20240412001355627.png" class="edkeccovawjg" alt="image-20240412001355627"></p><p>参考链接</p><hr><p><a href="https://www.cnblogs.com/d1012181765/p/15603378.html">https://www.cnblogs.com/d1012181765/p/15603378.html</a></p><p><a href="https://blog.csdn.net/qq_45297578/article/details/128723168">https://blog.csdn.net/qq_45297578/article/details/128723168</a></p>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Docker配置MySQL集群</title>
      <link href="/2024/04/01/Docker%E9%85%8D%E7%BD%AEMySQL%E9%9B%86%E7%BE%A4/"/>
      <url>/2024/04/01/Docker%E9%85%8D%E7%BD%AEMySQL%E9%9B%86%E7%BE%A4/</url>
      
        <content type="html"><![CDATA[<h1 id="Docker配置MySQL集群"><a href="#Docker配置MySQL集群" class="headerlink" title="Docker配置MySQL集群"></a>Docker配置MySQL集群</h1><h2 id="1、安装MySQL集群PXC镜像"><a href="#1、安装MySQL集群PXC镜像" class="headerlink" title="1、安装MySQL集群PXC镜像"></a><strong>1、安装MySQL集群PXC镜像</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull percona/percona-xtradb-cluster:5.7</span><br></pre></td></tr></table></figure><h2 id="2、创建docker内部网段"><a href="#2、创建docker内部网段" class="headerlink" title="2、创建docker内部网段"></a><strong>2、创建docker内部网段</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network create pxc-network</span><br></pre></td></tr></table></figure><h2 id="3、创建数据卷"><a href="#3、创建数据卷" class="headerlink" title="3、创建数据卷"></a><strong>3、创建数据卷</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">//对应五节点集群方案</span><br><span class="line">docker volume create v1</span><br><span class="line">docker volume create v2</span><br><span class="line">docker volume create v3</span><br><span class="line">docker volume create v4</span><br><span class="line">docker volume create v5</span><br></pre></td></tr></table></figure><h2 id="4、创建容器并运行"><a href="#4、创建容器并运行" class="headerlink" title="4、创建容器并运行"></a><strong>4、创建容器并运行</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//替换自己的mysql密码</span><br><span class="line">docker run -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=你设定的mysql密码 -e CLUSTER_NAME=cluster1 -v v1:/var/lib/mysql --name=node1 --net=pxc-network percona/percona-xtradb-cluster:5.7</span><br><span class="line"></span><br><span class="line">//主节点启动后经测试链接正常后，再创建其以下节点</span><br><span class="line">docker run -d -p 3307:3306 -e MYSQL_ROOT_PASSWORD=你设定的mysql密码 -e CLUSTER_NAME=cluster1 -e CLUSTER_JOIN=node1 -v v2:/var/lib/mysql --name=node2 --net=pxc-network \percona/percona-xtradb-cluster:5.7</span><br><span class="line"></span><br><span class="line">docker run -d -p 3308:3306 -e MYSQL_ROOT_PASSWORD=你设定的mysql密码 -e CLUSTER_NAME=cluster1 -e CLUSTER_JOIN=node1 -v v3:/var/lib/mysql --name=node3 --net=pxc-network \percona/percona-xtradb-cluster:5.7</span><br><span class="line"></span><br><span class="line">docker run -d -p 3309:3306 -e MYSQL_ROOT_PASSWORD=你设定的mysql密码 -e CLUSTER_NAME=cluster1 -e CLUSTER_JOIN=node1 -v v4:/var/lib/mysql --name=node4 --net=pxc-network \percona/percona-xtradb-cluster:5.7</span><br><span class="line"></span><br><span class="line">docker run -d -p 3310:3306 -e MYSQL_ROOT_PASSWORD=你设定的mysql密码 -e CLUSTER_NAME=cluster1 -e CLUSTER_JOIN=node1 -v v5:/var/lib/mysql --name=node5 --net=pxc-network \percona/percona-xtradb-cluster:5.7</span><br></pre></td></tr></table></figure><h2 id="5、云服务器安全组端口开放"><a href="#5、云服务器安全组端口开放" class="headerlink" title="5、云服务器安全组端口开放"></a><strong>5、云服务器安全组端口开放</strong></h2><p><style>.gdcvetppmgje{zoom:33%;}</style><img src="/2024/04/01/Docker%E9%85%8D%E7%BD%AEMySQL%E9%9B%86%E7%BE%A4/image-20240412132606413.png" class="gdcvetppmgje" alt="image-20240412132606413"></p><h2 id="6、检查运行情况"><a href="#6、检查运行情况" class="headerlink" title="6、检查运行情况"></a><strong>6、检查运行情况</strong></h2><h3 id="6-1-检查容器内运行情况"><a href="#6-1-检查容器内运行情况" class="headerlink" title="6.1 检查容器内运行情况"></a>6.1 检查容器内运行情况</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps -a</span><br></pre></td></tr></table></figure><p><img src="image-20240412133313368.png" alt="image-20240412133313368"></p><h3 id="6-2-检查数据库链接情况"><a href="#6-2-检查数据库链接情况" class="headerlink" title="6.2 检查数据库链接情况"></a>6.2 检查数据库链接情况</h3><p><style>.dklfbwhkmefi{zoom: 50%;}</style><img src="/2024/04/01/Docker%E9%85%8D%E7%BD%AEMySQL%E9%9B%86%E7%BE%A4/image-20240412133514624.png" class="dklfbwhkmefi" alt="image-20240412133514624"></p>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>如何将一个标量转换为与目标张量形状相同</title>
      <link href="/2023/05/15/%E5%A6%82%E4%BD%95%E5%B0%86%E4%B8%80%E4%B8%AA%E6%A0%87%E9%87%8F%E8%BD%AC%E6%8D%A2%E4%B8%BA%E4%B8%8E%E7%9B%AE%E6%A0%87%E5%BC%A0%E9%87%8F%E5%BD%A2%E7%8A%B6%E7%9B%B8%E5%90%8C/"/>
      <url>/2023/05/15/%E5%A6%82%E4%BD%95%E5%B0%86%E4%B8%80%E4%B8%AA%E6%A0%87%E9%87%8F%E8%BD%AC%E6%8D%A2%E4%B8%BA%E4%B8%8E%E7%9B%AE%E6%A0%87%E5%BC%A0%E9%87%8F%E5%BD%A2%E7%8A%B6%E7%9B%B8%E5%90%8C/</url>
      
        <content type="html"><![CDATA[<h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a><strong>背景</strong></h4><hr><p>参考别人的代码中，它直接将一个标量进行reshape，导致我报错了，但它可以正常运行，目前还不知道是什么原因，后面再看看。<br>在扩散模型的<strong>Forward process</strong>中，可以根据以下公式计算得到t时刻的带噪图像xt，其中αt是表示噪声比例的一个系数，是一个标量。在<strong>Pytorch</strong>中一个标量不能直接和一个不同维度的张量进行广播操作，所以需要将一个标量转换为与目标张量形状相同。</p><script type="math/tex; mode=display">x_t = \sqrt{ \bar \alpha_{t}}x_0 + \sqrt{1-\bar \alpha_{t}}\epsilon</script><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a><strong>方法</strong></h4><hr><p>假设x0维度- [N,C,H,W] -  [8,3,64,64] , 而αt维度为零维 , 所以想到了以下两种方式</p><p><strong>1、先unsqueeze，再repeat，最后循环unsqueeze</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqrt_alpha_t_cumprod = self.sqrt_alpha_cumprod[t].unsqueeze(-<span class="number">1</span>).repeat(batch_size)</span><br><span class="line">sqrt_one_minus_alpha_t_cumprod = self.sqrt_one_minus_alpha_cumprod[t].unsqueeze(-<span class="number">1</span>).repeat(batch_size)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(original_shape)-<span class="number">1</span>):</span><br><span class="line">    sqrt_alpha_t_cumprod = sqrt_alpha_t_cumprod.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">    sqrt_one_minus_alpha_t_cumprod = sqrt_one_minus_alpha_t_cumprod.unsqueeze(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>αt的维度变化 - [1] - [N] - [N,1] - [N,1,1] - [N,1,1,1]</p><p><strong>2、循环unsqueeze，expand_as</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqrt_alpha_t_cumprod = self.sqrt_alpha_cumprod[t]</span><br><span class="line">sqrt_one_minus_alpha_t_cumprod = self.sqrt_one_minus_alpha_cumprod[t]</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(original_shape)):</span><br><span class="line">    sqrt_alpha_t_cumprod = sqrt_alpha_t_cumprod.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">    sqrt_one_minus_alpha_t_cumprod = sqrt_one_minus_alpha_t_cumprod.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">sqrt_alpha_t_cumprod = sqrt_alpha_t_cumprod.expand_as(original)</span><br><span class="line">sqrt_one_minus_alpha_t_cumprod = sqrt_one_minus_alpha_t_cumprod.expand_as(noise)</span><br></pre></td></tr></table></figure><p>αt的维度变化 - [1]  - [1,1] - [1,1,1] - [1,1,1,1] - [8,3,64,64]</p><p><strong>3、作者的方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqrt_alpha_cumprod = self.sqrt_alpha_cumprod[t].reshape(batch_size)</span><br><span class="line">sqrt_one_minus_alpha_cumprod = self.sqrt_one_minus_alpha_cumprod[t].reshape(batch_size)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(original_shape)-<span class="number">1</span>):</span><br><span class="line">    sqrt_alpha_cumprod = sqrt_alpha_cumprod.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">    sqrt_one_minus_alpha_cumprod = sqrt_one_minus_alpha_cumprod.unsqueeze(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hexo加载公式问题</title>
      <link href="/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%85%AC%E5%BC%8F%E9%97%AE%E9%A2%98/"/>
      <url>/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%85%AC%E5%BC%8F%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/yexiaohhjk/article/details/82526604">配置方法参考</a></p><p>配置后正常显示公式</p><img src="/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%85%AC%E5%BC%8F%E9%97%AE%E9%A2%98/image-20230509153716208.png" class="" title="image-20230509153716208">]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hexo命令操作</title>
      <link href="/2023/05/09/hexo%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
      <url>/2023/05/09/hexo%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p><code>hexo new post &quot; &quot;</code>：创建一篇博客</p><p><code>hexo cl</code>：清理public文件夹</p><p><code>hexo g</code>：生成public文件夹</p><p><code>hexo s</code>：启动博客</p><p><code>hexo deploy</code>：本地修改同步至远端</p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hexo加载图片问题</title>
      <link href="/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/"/>
      <url>/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/476601594">配置方法参考</a></p><p><strong>1、配置图像根目录</strong></p><p><style>.bxonltsmsutg{zoom:50%;}</style><img src="/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/image-20230509151512393.png" class="bxonltsmsutg" alt="image-20230509151512393"></p><p><strong>2、删除图像名前面的斜杠</strong></p><p><style>.pocnbwdcbacg{zoom:67%;}</style><img src="/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/image-20230509151638033.png" class="pocnbwdcbacg" alt="image-20230509151638033"></p><p>参考</p><blockquote><p><a href="https://blog.csdn.net/z952957407/article/details/111642548">https://blog.csdn.net/z952957407/article/details/111642548</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
