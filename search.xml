<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>转置卷积（Transpose Convolution）</title>
      <link href="/2023/05/10/%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%EF%BC%88Transpose-Convolution%EF%BC%89/"/>
      <url>/2023/05/10/%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%EF%BC%88Transpose-Convolution%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>【1】</p><p>卷积的”逆”过程，是一种上采样操作</p><p><img src="Untitled.png" alt="Untitled" style="zoom:50%;" /></p><p>【2】</p><p>通过相对较小的特征图恢复原分辨率大小，可以用来对比学习这些参数的作用</p><p>【3】</p><p>双线性插值：</p><p>转置卷积：</p><script type="math/tex; mode=display">H_{out} = (H_{in} - 1) * S + K - 2P  + 1\tag{1}</script><script type="math/tex; mode=display">W_{out} = (W_{in} - 1) * S + K - 2P + 1\tag{2}</script><p>下采样时的卷积步长大于1，则反卷积时需要设置output_padding</p><script type="math/tex; mode=display">H_{out}=(H_{in}−1)×S−2P+dilation[0]×(K−1)+outputpadding[0]+1</script><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">16</span>, <span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line">downsample = nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, <span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">upsample = nn.ConvTranspose2d(<span class="number">16</span>, <span class="number">16</span>, <span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># torch.Size([1, 16, 6, 6])</span></span><br><span class="line">h = downsample(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># torch.Size([1, 16, 12, 12])</span></span><br><span class="line">output = upsample(h, output_size=<span class="built_in">input</span>.size())</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>普通卷积</title>
      <link href="/2023/05/10/%E6%99%AE%E9%80%9A%E5%8D%B7%E7%A7%AF/"/>
      <url>/2023/05/10/%E6%99%AE%E9%80%9A%E5%8D%B7%E7%A7%AF/</url>
      
        <content type="html"><![CDATA[<p>【1】</p><p>指的是作用于图像的二维卷积，它是一种可以保留邻域信息或下采样的操作</p><p>【2】</p><p>用于提取图像局部特征，如纹理，颜色，深度等</p><p>【3】</p><p>卷积核K：</p><p><img src="Untitled.png" alt="Untitled"></p><p>填充P：</p><p><img src="Untitled-16837099936742.png" alt="Untitled" style="zoom:33%;" /></p><p>步幅S：</p><p><img src="Untitled-16837100168054.png" alt="Untitled" style="zoom:50%;" /></p><p><img src="Untitled-16837100302296.png" alt="Untitled" style="zoom:50%;" /></p><script type="math/tex; mode=display">H_{out} = \frac{H_{in} + 2P - K}{S} + 1 \tag{1}</script><script type="math/tex; mode=display">W_{out} = \frac{W_{in} + 2P - K}{S} + 1 \tag{2}</script><p>注意到当P = K//2 时，保持特征图的大小不变，只改变通道数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">64</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">3</span>//<span class="number">2</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED问题</title>
      <link href="/2023/05/10/RuntimeError-cuDNN-error-CUDNN-STATUS-NOT-INITIALIZED%E9%97%AE%E9%A2%98/"/>
      <url>/2023/05/10/RuntimeError-cuDNN-error-CUDNN-STATUS-NOT-INITIALIZED%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>表示显存不足<br><code>nvidia-smi</code> 查看当前显存占用进程及其pid<br><code>kill</code> 清理一些的占用显存的进程或者改batch_size</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo加载公式问题</title>
      <link href="/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%85%AC%E5%BC%8F%E9%97%AE%E9%A2%98/"/>
      <url>/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%85%AC%E5%BC%8F%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/yexiaohhjk/article/details/82526604">配置方法参考</a></p><p>配置后正常显示公式</p><p><img src="image-20230509153716208.png" alt="image-20230509153716208"></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hexo命令操作</title>
      <link href="/2023/05/09/hexo%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
      <url>/2023/05/09/hexo%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p><code>hexo new post &quot; &quot;</code>：创建一篇博客</p><p><code>hexo cl</code>：清理public文件夹</p><p><code>hexo g</code>：生成public文件夹</p><p><code>hexo s</code>：启动博客</p><p><code>hexo deploy</code>：本地修改同步至远端</p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hexo加载图片问题</title>
      <link href="/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/"/>
      <url>/2023/05/09/hexo%E5%8A%A0%E8%BD%BD%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/476601594">配置方法参考</a></p><p><strong>1、配置图像根目录</strong></p><p><img src="image-20230509151512393.png" alt="image-20230509151512393" style="zoom:50%;" /></p><p><strong>2、删除图像名前面的斜杠</strong></p><p><img src="image-20230509151638033.png" alt="image-20230509151638033" style="zoom:67%;" /></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>反向传播</title>
      <link href="/2023/05/09/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
      <url>/2023/05/09/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
      
        <content type="html"><![CDATA[<p>【1】</p><p>是一种更新训练参数（权重）的技术</p><p>【2】</p><p>从后向前逐层更新参数，但是后层的梯度值需要前层计算所得</p><p>【3】</p><p><img src="image-20230509135540661.png" alt="image-20230509135540661" style="zoom:50%;" /></p><script type="math/tex; mode=display">\begin{aligned}\sigma (·) &: Activation \, function \\f(·)&: Fully\,connected\,layer \\x^l_j &: 第l层上的第j个结点的值 \\ w_{ji}^{l} &: 第l-1层的第i个节点到第l层的第j个结点的权重\end{aligned}\tag{0}</script><script type="math/tex; mode=display">\begin{aligned}x^l_j &= \sum_{i=1} ^{m} w_{ji}^{l}y_{i}^{l-1} +  \theta_{j}^{l} \end{aligned} \tag{1}</script><script type="math/tex; mode=display">\begin{aligned}y_{j} ^ {l} &= \sigma (x_{j}^{l}) \end{aligned} \tag{2}</script><script type="math/tex; mode=display">\bar{y} = f(\sum_{j=1}^{n}y_{j}) \tag{3}</script><script type="math/tex; mode=display">\mathcal L = |y - \bar{y}| \tag{4}</script><p>改变某一个位置的权重，对整体损失函数的影响</p><script type="math/tex; mode=display">\begin{aligned}\frac{ \partial \mathcal L}{\partial w_{ji}^{l}} &= \frac{\partial \mathcal L}{\partial x^l_j} \frac{\partial x^l_j}{\partial w_{ji}^{l}}\end{aligned} \tag{5}</script><p>因此这种连乘的形式给深层网络带来了梯度消失/爆炸问题</p><script type="math/tex; mode=display">\begin{aligned}w_{ji}^{l} = w_{ji}^{l}  - \mathcal \alpha \frac{\partial \mathcal L}{\partial w_{ji}^{l}}\end{aligned} \tag{6}</script><p>导致最终很难持续学习下去</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
